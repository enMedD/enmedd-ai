FROM python:3.11.7-slim-bookworm

ENV ENMEDD_RUNNING_IN_DOCKER="true"

COPY ./requirements/model_server.txt /tmp/requirements.txt
RUN pip install --no-cache-dir --upgrade \
        --retries 5 \
        --timeout 30 \
        -r /tmp/requirements.txt

RUN apt-get remove -y --allow-remove-essential perl-base && \
    apt-get autoremove -y

# Pre-downloading models for setups with limited egress
# Download tokenizers, distilbert for the enMedD AI model
# Download model weights
# Run Nomic to pull in the custom architecture and have it cached locally
RUN python -c "from transformers import AutoTokenizer; \
AutoTokenizer.from_pretrained('distilbert-base-uncased'); \
AutoTokenizer.from_pretrained('mixedbread-ai/mxbai-rerank-xsmall-v1'); \
from huggingface_hub import snapshot_download; \
snapshot_download(repo_id='danswer/hybrid-intent-token-classifier', revision='v1.0.3'); \
snapshot_download('nomic-ai/nomic-embed-text-v1'); \
snapshot_download('mixedbread-ai/mxbai-rerank-xsmall-v1'); \
from sentence_transformers import SentenceTransformer; \
SentenceTransformer(model_name_or_path='nomic-ai/nomic-embed-text-v1', trust_remote_code=True);"

# In case the user has volumes mounted to /root/.cache/huggingface that they've downloaded while
# running enMedD AI, don't overwrite it with the built in cache folder
RUN mv /root/.cache/huggingface /root/.cache/temp_huggingface

WORKDIR /app

# Utils used by model server
COPY ./enmedd/utils/logger.py /app/enmedd/utils/logger.py

# Place to fetch version information
COPY ./enmedd/__init__.py /app/enmedd/__init__.py

# Shared between enMedD AI Backend and Model Server
COPY ./shared_configs /app/shared_configs

# Model Server main code
COPY ./model_server /app/model_server

ENV PYTHONPATH=/app

EXPOSE 9000

CMD ["uvicorn", "model_server.main:app", "--host", "0.0.0.0", "--port", "9000"]
